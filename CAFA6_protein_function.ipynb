{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d05973",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_sequences.fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 1. Load Training Data\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load training sequences\u001b[39;00m\n\u001b[32m     21\u001b[39m train_sequences_file = os.path.join(DATA_DIR, \u001b[33m'\u001b[39m\u001b[33mtrain_sequences.fasta\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m train_sequences = {record.id: \u001b[38;5;28mstr\u001b[39m(record.seq) \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSeqIO\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sequences_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfasta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m}\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of training sequences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_sequences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Load GO terms\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/Bio/SeqIO/__init__.py:626\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(handle, format, alphabet)\u001b[39m\n\u001b[32m    624\u001b[39m iterator_generator = _FormatToIterator.get(\u001b[38;5;28mformat\u001b[39m)\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iterator_generator:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m AlignIO._FormatToIterator:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# Use Bio.AlignIO to read in the alignments\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (r \u001b[38;5;28;01mfor\u001b[39;00m alignment \u001b[38;5;129;01min\u001b[39;00m AlignIO.parse(handle, \u001b[38;5;28mformat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m alignment)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/Bio/SeqIO/FastaIO.py:196\u001b[39m, in \u001b[36mFastaIterator.__init__\u001b[39m\u001b[34m(self, source, alphabet)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m alphabet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe alphabet argument is no longer supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFasta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    198\u001b[39m     line = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/Bio/SeqIO/Interfaces.py:81\u001b[39m, in \u001b[36mSequenceIterator.__init__\u001b[39m\u001b[34m(self, source, alphabet, fmt)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, _PathLikeTypes):\n\u001b[32m     80\u001b[39m     mode = modes[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     83\u001b[39m     value = source.read(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/train_sequences.fasta'"
     ]
    }
   ],
   "source": [
    "# CAFA 6 Protein Function Prediction\n",
    "# Notebook for data loading, preprocessing, and preparing for model building\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import obonet\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set data folder path\n",
    "DATA_DIR = 'data/'\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load Training Data\n",
    "# -------------------------------\n",
    "\n",
    "# Load training sequences\n",
    "train_sequences_file = os.path.join(DATA_DIR, 'train_sequences.fasta')\n",
    "train_sequences = {record.id: str(record.seq) for record in SeqIO.parse(train_sequences_file, \"fasta\")}\n",
    "print(f\"Number of training sequences: {len(train_sequences)}\")\n",
    "\n",
    "# Load GO terms\n",
    "train_terms_file = os.path.join(DATA_DIR, 'train_terms.tsv')\n",
    "train_terms = pd.read_csv(train_terms_file, sep='\\t', header=None, names=['Protein', 'GO', 'Ontology'])\n",
    "print(f\"Number of annotated GO terms: {len(train_terms)}\")\n",
    "train_terms.head()\n",
    "\n",
    "# Load taxonomy\n",
    "train_taxonomy_file = os.path.join(DATA_DIR, 'train_taxonomy.tsv')\n",
    "train_taxonomy = pd.read_csv(train_taxonomy_file, sep='\\t', header=None, names=['Protein', 'TaxonID'])\n",
    "train_taxonomy.head()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load Gene Ontology Graph\n",
    "# -------------------------------\n",
    "go_file = os.path.join(DATA_DIR, 'go-basic.obo')\n",
    "graph = obonet.read_obo(go_file)\n",
    "print(f\"Number of GO terms in graph: {len(graph)}\")\n",
    "\n",
    "# Example: visualize a small part of GO graph\n",
    "sub_nodes = list(graph.nodes())[:10]\n",
    "subgraph = graph.subgraph(sub_nodes)\n",
    "nx.draw(subgraph, with_labels=True, node_size=2000, node_color='skyblue')\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Information Accretion\n",
    "# -------------------------------\n",
    "ia_file = os.path.join(DATA_DIR, 'IA.tsv')\n",
    "ia = pd.read_csv(ia_file, sep='\\t', header=None, names=['GO', 'IA'])\n",
    "ia.head()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Load Test Superset\n",
    "# -------------------------------\n",
    "test_sequences_file = os.path.join(DATA_DIR, 'testsuperset.fasta')\n",
    "test_sequences = {record.id: str(record.seq) for record in SeqIO.parse(test_sequences_file, \"fasta\")}\n",
    "print(f\"Number of test sequences: {len(test_sequences)}\")\n",
    "\n",
    "# Load test taxon IDs\n",
    "test_taxon_file = os.path.join(DATA_DIR, 'testsuperset-taxon-list.tsv')\n",
    "test_taxons = pd.read_csv(test_taxon_file, sep='\\t', header=None, names=['Protein', 'TaxonID'])\n",
    "test_taxons.head()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Prepare Data for Modeling\n",
    "# -------------------------------\n",
    "\n",
    "# Example: mapping proteins to their GO terms\n",
    "protein_to_go = train_terms.groupby('Protein')['GO'].apply(list).to_dict()\n",
    "\n",
    "# Quick check for a sample protein\n",
    "sample_protein = list(train_sequences.keys())[0]\n",
    "print(\"Protein ID:\", sample_protein)\n",
    "print(\"Sequence:\", train_sequences[sample_protein])\n",
    "print(\"GO terms:\", protein_to_go.get(sample_protein, []))\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Sample Submission Format\n",
    "# -------------------------------\n",
    "sample_submission_file = os.path.join(DATA_DIR, 'sample_submission.tsv')\n",
    "sample_submission = pd.read_csv(sample_submission_file, sep='\\t', header=None)\n",
    "sample_submission.head()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Optional: Encode Sequences for ML/DL\n",
    "# -------------------------------\n",
    "# Example: simple one-hot encoding for amino acids\n",
    "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_int = {aa: idx for idx, aa in enumerate(amino_acids)}\n",
    "\n",
    "def one_hot_encode(seq, max_len=1000):\n",
    "    encoding = np.zeros((max_len, len(amino_acids)), dtype=int)\n",
    "    for i, aa in enumerate(seq[:max_len]):\n",
    "        if aa in aa_to_int:\n",
    "            encoding[i, aa_to_int[aa]] = 1\n",
    "    return encoding\n",
    "\n",
    "# Encode a sample sequence\n",
    "encoded_seq = one_hot_encode(train_sequences[sample_protein])\n",
    "print(\"Encoded shape:\", encoded_seq.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Notebook ready for model building\n",
    "# Next steps:\n",
    "# 1. Convert GO terms to multi-label vectors\n",
    "# 2. Train machine learning / deep learning models (e.g., transformers, CNN, RNN)\n",
    "# 3. Make predictions on test sequences\n",
    "# 4. Generate submission file\n",
    "# -------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
